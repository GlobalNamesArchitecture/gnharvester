{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Homo sapiens', 'Salinator solida', 'Taraxacum officinale']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "namespace = uuid.UUID('90181196-fecf-5082-a4c1-411d4f314cda')\n",
    "empty_uuid = uuid.uuid5(namespace, \"\")\n",
    "\n",
    "def parse_spark(names):\n",
    "    from pyspark.mllib.common import _py2java, _java2py\n",
    "    parser = sc._jvm.org.globalnames.parser.spark.Parser()\n",
    "    result = parser.parse(_py2java(sc, names), False, True)\n",
    "    return _java2py(sc, result)\n",
    "\n",
    "names = sc.parallelize([\"Homo sapiens Linnaeus 1758\",\n",
    "                        \"Salinator solida (Martens, 1878)\",\n",
    "                        \"Taraxacum officinale F. H. Wigg.\"])\n",
    "parse_spark(names).map(lambda x: json.loads(x)[\"canonical_name\"][\"value\"]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvs_dir = 'csvs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Using a password on the command line interface can be insecure.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p mysql-export\n",
    "mysql -B -h $MYSQL_HOST -u$MYSQL_USER --password=$MYSQL_PASS $MYSQL_DB \\\n",
    "    -e \"SELECT id, \\\n",
    "               REPLACE(REPLACE(REPLACE(name, '\\r\\n', ' '), '\\n', ' '), '\\t', ' ') as name, \\\n",
    "               REPLACE(REPLACE(REPLACE(normalized, '\\r\\n', ' '), '\\n', ' '), '\\t', ' ') as normalized, \\\n",
    "               canonical_form_id, \\\n",
    "               has_words, \\\n",
    "               uuid, \\\n",
    "               has_groups, \\\n",
    "               surrogate \\\n",
    "        FROM name_strings \\\n",
    "        -- where id in (38990431, 78688931, 38618228, 83329994, 38618226, 83329995, 38618227, 83329996, 38619074, 81729304) \\\n",
    "        -- LIMIT 1000000\" 1> mysql-export/name_strings.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(os.path.join(\"mysql-export\", \"name_strings.tsv\"), header=True, quote=\"\", sep=\"\\t\")\n",
    "\n",
    "names = df.rdd.map(lambda x: x[\"name\"])\n",
    "\n",
    "parsed_names_json = parse_spark(names) \\\n",
    "  .map(lambda result: json.loads(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csvs/name_strings\n",
      "CPU times: user 200 ms, sys: 12 ms, total: 212 ms\n",
      "Wall time: 17min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "output_dir_canonicals = os.path.join(csvs_dir, \"name_strings\")\n",
    "\n",
    "! rm -rf $output_dir_canonicals\n",
    "\n",
    "print(output_dir_canonicals)\n",
    "\n",
    "def extract_name_strings_fields(result_json):\n",
    "    name_string_id = result_json[\"name_string_id\"]\n",
    "    verbatim = result_json[\"verbatim\"]\n",
    "    if result_json[\"parsed\"]:\n",
    "        canonical_name = result_json[\"canonical_name\"][\"value\"]\n",
    "        canonical_name_uuid = result_json[\"canonical_name\"][\"id\"]\n",
    "    else:\n",
    "        canonical_name = \"\"\n",
    "        canonical_name_uuid = str(empty_uuid)\n",
    "    return \"\\t\".join([name_string_id, verbatim, canonical_name_uuid, canonical_name])\n",
    "\n",
    "canonical_names = parsed_names_json \\\n",
    "    .map(extract_name_strings_fields) \\\n",
    "    .distinct() \\\n",
    "    .saveAsTextFile(output_dir_canonicals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: author_word\n",
      "processing: genus\n",
      "processing: uninomial\n",
      "processing: year\n",
      "processing: approximate_year\n",
      "processing: infrageneric_epithet\n",
      "processing: specific_epithet\n",
      "CPU times: user 1.33 s, sys: 256 ms, total: 1.58 s\n",
      "Wall time: 2h 11min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word_targets = [\"author_word\", \"genus\", \"uninomial\", \"year\", \n",
    "                \"approximate_year\", \"infrageneric_epithet\", \"specific_epithet\"]\n",
    "\n",
    "def extract_word_indexes(parsed_name_json, word_target):\n",
    "    verbatim = parsed_name_json[\"verbatim\"]\n",
    "    name_string_id = parsed_name_json[\"name_string_id\"]\n",
    "    words_pos = filter(lambda j: j[0] == word_target, parsed_name_json[\"positions\"])\n",
    "    words = map(lambda word_pos: verbatim[word_pos[1]:word_pos[2]], words_pos)\n",
    "    return map(lambda word: \"\\t\".join([name_string_id, word]), words)\n",
    "\n",
    "for word_target in word_targets:\n",
    "    print(\"processing:\", word_target)\n",
    "    output_dir_word_target = os.path.join(csvs_dir, \"word_indexes\", word_target)\n",
    "    \n",
    "    ! rm -rf $output_dir_word_target\n",
    "    \n",
    "    parsed_names_json \\\n",
    "      .filter(lambda parsed_name: \"positions\" in parsed_name) \\\n",
    "      .flatMap(lambda parsed_name: extract_word_indexes(parsed_name, word_target)) \\\n",
    "      .distinct() \\\n",
    "      .saveAsTextFile(output_dir_word_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 24: unexpected EOF while looking for matching `''\n",
      "bash: line 28: syntax error: unexpected end of file\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p mysql-export\n",
    "mysql -B -h $MYSQL_HOST -u$MYSQL_USER --password=$MYSQL_PASS $MYSQL_DB \\\n",
    "    -e \"SELECT \\\n",
    "        nsi.data_source_id , \\\n",
    "        ns.id , \\\n",
    "        REPLACE( \\\n",
    "          REPLACE( \\\n",
    "            REPLACE(name , '\\r\\n' , ' ') , \\\n",
    "            '\\n' , \\\n",
    "            ' ' \\\n",
    "          ) , \\\n",
    "          '\\t' , \\\n",
    "          ' ' \\\n",
    "        ) AS name , \\\n",
    "        REPLACE( \\\n",
    "          REPLACE( \\\n",
    "            REPLACE(url , '\\r\\n' , ' ') , \\\n",
    "            '\\n' , \\\n",
    "            ' ' \\\n",
    "          ) , \\\n",
    "          '\\t' , \\\n",
    "          ' ' \\\n",
    "        ) AS url \\\n",
    "      FROM name_string_indices AS nsi \\\n",
    "      JOIN name_strings AS ns ON nsi.name_string_id = ns.id \\\n",
    "      -- LIMIT 1000\" 1> mysql-export/name_string_indices.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(os.path.join(\"mysql-export\", \"name_string_indices.tsv\"), header=True, sep='\\t')\n",
    "\n",
    "names = df.rdd.map(lambda x: x[\"name\"])\n",
    "\n",
    "output_dir_name_string_indices = os.path.join(csvs_dir, \"name_string_indices\")\n",
    "\n",
    "! rm -rf $output_dir_name_string_indices\n",
    "\n",
    "def name_string_index_result(value):\n",
    "    result_json, name_string_row = value\n",
    "    name_string_id = result_json[\"name_string_id\"]\n",
    "    url = name_string_row[\"url\"].replace(\"\\t\", \" \")\n",
    "    return \"\\t\".join([name_string_row[\"data_source_id\"], name_string_id, url])\n",
    "\n",
    "parsed_names_json = parse_spark(names) \\\n",
    "  .map(lambda result: json.loads(result)) \\\n",
    "  .zip(df.rdd) \\\n",
    "  .map(name_string_index_result)\n",
    "\n",
    "parsed_names_json.saveAsTextFile(output_dir_name_string_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}